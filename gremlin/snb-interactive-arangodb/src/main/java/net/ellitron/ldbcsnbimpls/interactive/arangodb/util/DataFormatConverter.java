/* 
 * Copyright (C) 2019 Stanford University
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */
package net.ellitron.ldbcsnbimpls.interactive.arangodb.util;

import org.docopt.Docopt;

import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.nio.file.NoSuchFileException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Calendar;
import java.util.Collections;
import java.util.Date;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.TimeZone;

/**
 * A utility for converting dataset files generated by the LDBC SNB Data
 * Generator[1] to a format usable by the ArangoDB import tool. Mainly
 * this converts date and datetimes to time-since-epochs, since at the 
 * time of writing ArangoDB does not support any other data types besides
 * String and Integer.
 *
 * [1]: git@github.com:ldbc/ldbc_snb_datagen.git<br>
 *
 * @author Jonathan Ellithorpe (jde@cs.stanford.edu)
 */
public class DataFormatConverter {

  private static final String doc =
      "DataFormatConverter: A utility for converting dataset files generated "
      + "by the LDBC SNB Data Generator to a format usable by the "
      + "ArangoDB import tool."
      + "\n"
      + "Usage:\n"
      + "  DataFormatConverter SOURCE DEST\n"
      + "  DataFormatConverter (-h | --help)\n"
      + "  DataFormatConverter --version\n"
      + "\n"
      + "Arguments:\n"
      + "  SOURCE  Directory containing SNB dataset files.\n"
      + "  DEST    Destination directory for output files.\n"
      + "\n"
      + "Options:\n"
      + "  -h --help         Show this screen.\n"
      + "  --version         Show version.\n"
      + "\n";

  /**
   * Represents all the types of nodes in the graph and their various
   * properties.
   */
  private enum Node {

    COMMENT("Comment", "comment",
        new String[]{"creationDate", "locationIP", "browserUsed", "content", "length"}),
    FORUM("Forum", "forum",
        new String[]{"title", "creationDate"}),
    ORGANISATION("Organisation", "organisation",
        new String[]{"type", "name", "url"}),
    PERSON("Person", "person",
        new String[]{"firstName", "lastName", "gender", "birthday", "creationDate", 
          "locationIP", "browserUsed", "email", "speaks"}),
    PLACE("Place", "place",
        new String[]{"name", "url", "type"}),
    POST("Post", "post",
        new String[]{"imageFile", "creationDate", "locationIP", "browserUsed", 
          "language", "content", "length"}),
    TAG("Tag", "tag",
        new String[]{"name", "url"}),
    TAGCLASS("TagClass", "tagclass",
        new String[]{"name", "url"});

    /*
     * The label given to these nodes.
     */
    private final String label;

    /*
     * This is the name used in fileNames for the node as output by the LDBC
     * SNB Data Generator.
     */
    private final String fileTag;

    /*
     * Ordered array of the properties for this node, in the order they appear
     * in the LDBC SNB Data Generator dataset files.
     */
    private final String[] props;

    private Node(String label, String fileTag, String[] props) {
      this.label = label;
      this.fileTag = fileTag;
      this.props = props;
    }

    public String getLabel() {
      return label;
    }

    public String getFileTag() {
      return fileTag;
    }

    public String[] getProps() {
      return props;
    }
  }

  /**
   * Represents all the types of relationships in the graph.
   */
  private enum Relationship {

    CONTAINEROF("containerOf", new String[]{}),
    HASCREATOR("hasCreator", new String[]{}),
    HASINTEREST("hasInterest", new String[]{}),
    HASMEMBER("hasMember", new String[]{"joinDate"}),
    HASMODERATOR("hasModerator", new String[]{}),
    HASTAG("hasTag", new String[]{}),
    HASTYPE("hasType", new String[]{}),
    ISLOCATEDIN("isLocatedIn", new String[]{}),
    ISPARTOF("isPartOf", new String[]{}),
    ISSUBCLASSOF("isSubclassOf", new String[]{}),
    KNOWS("knows", new String[]{"creationDate"}),
    LIKES("likes", new String[]{"creationDate"}),
    REPLYOF("replyOf", new String[]{}),
    SPEAKS("speaks", new String[]{}),
    STUDYAT("studyAt", new String[]{"classYear"}),
    WORKAT("workAt", new String[]{"workFrom"});

    /*
     * The named used in LDBC SNB Data Generator output files referring to this
     * type of relationship.
     */
    private final String fileTag;

    /*
     * Ordered array of the properties for this relationship type, in the order
     * they appear in the LDBC SNB Data Generator dataset files.
     */
    private final String[] props;

    private Relationship(String fileTag, String[] props) {
      this.fileTag = fileTag;
      this.props = props;
    }

    public String getFileTag() {
      return fileTag;
    }

    public String[] getProps() {
      return props;
    }
  }

  /*
   * Stores a fixed mapping between property names and their data types.
   */
  private static final Map<String, String> propDataTypes;

  /*
   * Used for parsing dates in the original dataset files output by the data
   * generator, and converting them to milliseconds since Jan. 1 9170. We store
   * dates in this form.
   */
  private static final SimpleDateFormat birthdayDateFormat;
  private static final SimpleDateFormat creationDateDateFormat;
  private static final Calendar cal;

  static {
    Map<String, String> dataTypeMap = new HashMap<>();
    dataTypeMap.put("birthday", "long");
    dataTypeMap.put("browserUsed", "string");
    dataTypeMap.put("classYear", "int");
    dataTypeMap.put("content", "string");
    dataTypeMap.put("creationDate", "long");
    dataTypeMap.put("email", "string[]");
    dataTypeMap.put("firstName", "string");
    dataTypeMap.put("gender", "string");
    dataTypeMap.put("imageFile", "string");
    dataTypeMap.put("joinDate", "long");
    dataTypeMap.put("language", "string");
    dataTypeMap.put("lastName", "string");
    dataTypeMap.put("length", "int");
    dataTypeMap.put("locationIP", "string");
    dataTypeMap.put("name", "string");
    dataTypeMap.put("speaks", "string[]");
    dataTypeMap.put("title", "string");
    dataTypeMap.put("type", "string");
    dataTypeMap.put("url", "string");
    dataTypeMap.put("workFrom", "int");

    propDataTypes = Collections.unmodifiableMap(dataTypeMap);

    birthdayDateFormat =
        new SimpleDateFormat("yyyy-MM-dd");
    birthdayDateFormat.setTimeZone(TimeZone.getTimeZone("GMT"));
    creationDateDateFormat =
        new SimpleDateFormat("yyyy-MM-dd'T'HH:mm:ss.SSSZ");
    creationDateDateFormat.setTimeZone(TimeZone.getTimeZone("GMT"));
    cal = Calendar.getInstance(TimeZone.getTimeZone("GMT"));
  }

  /**
   * Parse a property file to coalesce multiple properties for a single id into
   * a list of those properties for the id. Used for parsing the email and
   * speaks property files for person nodes, but can be used on any node
   * property file with the same format.
   *
   * @param path Path to the property file.
   *
   * @return Map from node id to List of property values for the property
   * represented by the parsed file.
   *
   * @throws IOException
   */
  private static Map<String, List<String>> parsePropFile(List<Path> paths)
      throws IOException {
    Map<String, List<String>> propMap = new HashMap<>();

    for (Path path : paths) {
      BufferedReader propFile =
          Files.newBufferedReader(path, StandardCharsets.UTF_8);

      String line;
      propFile.readLine(); // Skip over the first line (column headers).
      while ((line = propFile.readLine()) != null) {
        String[] lineParts = line.split("\\|");
        String id = lineParts[0];
        String prop = lineParts[1];
        if (propMap.containsKey(id)) {
          propMap.get(id).add(prop);
        } else {
          List<String> list = new ArrayList<>();
          list.add(prop);
          propMap.put(id, list);
        }
      }
      propFile.close();
    }

    return propMap;
  }

  /**
   * Serialize a list of property values into a single String.
   *
   * @param propList List of property values.
   *
   * @return Serialized string of property values.
   */
  private static String serializePropertyValueList(List<String> propList) {
    StringBuilder sb = new StringBuilder();
    sb.append("\"");
    for (int i = 0; i < propList.size(); i++) {
      // If not first element, start with array separator
      if (i > 0) {
        sb.append(";");
      }

      sb.append(propList.get(i));
    }
    sb.append("\"");

    return sb.toString();
  }

  public static void main(String[] args)
      throws FileNotFoundException, IOException, ParseException {
    Map<String, Object> opts =
        new Docopt(doc).withVersion("DataFormatConverter 1.0").parse(args);

    /*
     * Save the output file names. We'll use this later to output a script
     * containing the full arangoimport command to import all the files
     * converted by this utility.
     */
    List<String> outputNodeFiles = new ArrayList<>();
    List<String> outputRelFiles = new ArrayList<>();

    String inputDir = (String) opts.get("SOURCE");
    String outputDir = (String) opts.get("DEST");

    System.out.println(String.format("Processing person properties..."));

    // Prepare email and speaks properties to be added to Person nodes.
    File allFiles[] = (new File(inputDir)).listFiles();
    List<Path> emailaddressPropFilePaths = new ArrayList<>();
    List<Path> languagePropFilePaths = new ArrayList<>();
    for (int i = 0; i < allFiles.length; i++) {
      if (allFiles[i].isFile()) {
        if (allFiles[i].getName().matches(Node.PERSON.getFileTag() + "_email_emailaddress_\\d+_0.csv"))
          emailaddressPropFilePaths.add(allFiles[i].toPath());
        else if (allFiles[i].getName().matches(Node.PERSON.getFileTag() + "_speaks_language_\\d+_0.csv"))
          languagePropFilePaths.add(allFiles[i].toPath());
      }
    }

    Map<String, List<String>> personEmail = parsePropFile(emailaddressPropFilePaths);

    Map<String, List<String>> personSpeaks = parsePropFile(languagePropFilePaths);

    /*
     * Nodes.
     */
    for (Node node : Node.values()) {
      // First we gather up a list of the files for this node type.
      List<String> fileNames = new ArrayList<>();
      for (int i = 0; i < allFiles.length; i++) {
        if (allFiles[i].isFile()) {
          if (allFiles[i].getName().matches(node.getFileTag() + "_\\d+_0.csv"))
            fileNames.add(allFiles[i].getName());
        }
      }

      System.out.println(String.format("Processing %s nodes (%d files)...", 
          node.getFileTag(), fileNames.size()));

      for (String fileName : fileNames) {
        Path path = Paths.get(inputDir + "/" + fileName);
        BufferedReader inFile =
            Files.newBufferedReader(path, StandardCharsets.UTF_8);

        path = Paths.get(outputDir + "/" + fileName);
        BufferedWriter outFile =
            Files.newBufferedWriter(path, StandardCharsets.UTF_8);

        outputNodeFiles.add(fileName);

        // Grab the original header.
        String header = inFile.readLine();

        // Output new header.
        String[] headerFields = header.split("\\|");
        for (int i = 0; i < headerFields.length; i++) {
          outFile.append(headerFields[i]);
          if (headerFields[i].equals("birthday")) {
            outFile.append("|birthday_day");
            outFile.append("|birthday_month");
          }

          if (i != headerFields.length - 1)
            outFile.append("|");
          else {
            if (node.equals(Node.PERSON))
              outFile.append("|email|speaks");
            else if (node.equals(Node.COMMENT) || node.equals(Node.POST))
              outFile.append("|type");
            outFile.append("\n");
          }
        }

        /*
        * Now go through every line of the file processing certain columns,
        * adding fields as necessary.
        */
        String line;
        while ((line = inFile.readLine()) != null) {
          /*
          * Date-type fields (birthday, creationDate, ...) need to be converted
          * to the number of milliseconds since January 1, 1970, 00:00:00 GMT.
          * This is the format expected to be returned for these fields by LDBC
          * SNB benchmark queries, although the format in the dataset files are
          * things like "1989-12-04" and "2010-03-17T23:32:10.447+0000". We
          * could do this conversion "live" during the benchmark, but that would
          * detract from the performance numbers' reflection of true database
          * performance since it would add to the query processing overhead.
          */
          String[] colVals = line.split("\\|");
          for (int i = 0; i < headerFields.length; i++) {
            if (headerFields[i].equals("birthday")) {
              Date birthday = birthdayDateFormat.parse(colVals[i]);
              cal.setTime(birthday);
              outFile.append(String.valueOf(cal.getTimeInMillis()) + "|");
              outFile.append(String.valueOf(cal.get(Calendar.DAY_OF_MONTH)) + "|");
              outFile.append(String.valueOf(cal.get(Calendar.MONTH) + 1));
            } else if (headerFields[i].equals("creationDate") ||
                         headerFields[i].equals("joinDate")) {
              outFile.append(String.valueOf(
                  creationDateDateFormat.parse(colVals[i]).getTime()));
            } else {
              outFile.append(colVals[i]);
            }

            if (i != headerFields.length - 1)
              outFile.append("|");
            else {
              /*
              * For person nodes we merge their email and speaks properties listed
              * in their respective property files into the person node file as the
              * last properties.
              */
              if (node.equals(Node.PERSON)) {
                String id = colVals[0];

                // First append emails.
                outFile.append("|");
                if (personEmail.containsKey(id)) {
                  String email = serializePropertyValueList(personEmail.get(id));
                  outFile.append(email);
                }

                // Then append languages this person speaks.
                outFile.append("|");
                if (personSpeaks.containsKey(id)) {
                  String speaks = serializePropertyValueList(personSpeaks.get(id));
                  outFile.append(speaks);
                }
              } else if (node.equals(Node.COMMENT)) {
                outFile.append("|Comment");
              } else if (node.equals(Node.POST)) {
                outFile.append("|Post");
              }
              outFile.append("\n");
            }
          }
        }

        inFile.close();
        outFile.close();
      }
    }

    /*
     * Relationships.
     */
    for (Node srcNode : Node.values()) {
      for (Relationship rel : Relationship.values()) {
        for (Node dstNode : Node.values()) {
          List<String> fileNames = new ArrayList<>();
          for (int i = 0; i < allFiles.length; i++) {
            if (allFiles[i].isFile()) {
              if (allFiles[i].getName().matches(srcNode.getFileTag() 
                    + "_" + rel.getFileTag() 
                    + "_" + dstNode.getFileTag() 
                    + "_\\d+_0.csv"))
                fileNames.add(allFiles[i].getName());
            }
          }

          if (fileNames.size() == 0) {
            // No relationships of this type.
            continue;
          }

          System.out.println(String.format(
              "Processing (%s)-[%s]->(%s) relationships (%d files)...",
              srcNode.getFileTag(),
              rel.getFileTag(),
              dstNode.getFileTag(),
              fileNames.size()));

          for (String fileName : fileNames) {
            Path path = Paths.get(inputDir + "/" + fileName);

            BufferedReader inFile = 
                Files.newBufferedReader(path, StandardCharsets.UTF_8);

            path = Paths.get(outputDir + "/" + fileName);
            BufferedWriter outFile =
                Files.newBufferedWriter(path, StandardCharsets.UTF_8);

            outputRelFiles.add(fileName);

            // Grab the original header.
            String header = inFile.readLine();

            // Output new header.
            String[] headerFields = header.split("\\|");
            for (int i = 0; i < headerFields.length; i++) {
              if (i == 0)
                outFile.append("src.id");
              else if (i == 1)
                outFile.append("dst.id");
              else
                outFile.append(headerFields[i]);

              if (i != headerFields.length - 1)
                outFile.append("|");
              else
                outFile.append("\n");
            }

            /*
            * Now go through every line of the file processing certain columns.
            */
            String line;
            while ((line = inFile.readLine()) != null) {
              /*
              * Date-type fields (creationDate, joinDate...) need to be
              * converted to the number of milliseconds since January 1, 1970,
              * 00:00:00 GMT. This is the format expected to be returned for
              * these fields by LDBC SNB benchmark queries, although the format
              * in the dataset files are things like
              * "2010-03-17T23:32:10.447+0000". We could do this conversion
              * "live" during the benchmark, but that would detract from the
              * performance numbers' reflection of true database performance
              * since it would add to the client-side query processing overhead.
              */
              String[] colVals = line.split("\\|");
              for (int i = 0; i < headerFields.length; i++) {
                if (headerFields[i].equals("creationDate") ||
                        headerFields[i].equals("joinDate")) {
                  outFile.append(String.valueOf(
                      creationDateDateFormat.parse(colVals[i]).getTime()));
                } else
                  outFile.append(colVals[i]);

                if (i != headerFields.length - 1)
                  outFile.append("|");
                else
                  outFile.append("\n");
              }
            }

            inFile.close();
            outFile.close();
          }
        }
      }
    }
  }
}
